{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install kfp --upgrade --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp \n",
    "\n",
    "import kfp.dsl as dsl \n",
    "\n",
    "from kfp import compiler \n",
    "\n",
    "from kfp import components \n",
    "\n",
    "from kfp.aws import use_aws_secret \n",
    "\n",
    " \n",
    "\n",
    "BASE_IMAGE = 'tensorflow/tensorflow:2.0.0b0-py3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component( \n",
    "\n",
    "    name='data_process_op', \n",
    "\n",
    "    description='process data', \n",
    "\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step.  \n",
    "\n",
    ") \n",
    "\n",
    "def process_data(glue_job_name: str, region: str ) -> str: \n",
    "\n",
    "    import os \n",
    "\n",
    "    import boto3 \n",
    "\n",
    "    import time \n",
    "\n",
    "     \n",
    "\n",
    "    print ('start data processing') \n",
    "\n",
    "     \n",
    "\n",
    "    # kick off the Glue Job to process data \n",
    "\n",
    "    client = boto3.client('glue', region_name= region) \n",
    "\n",
    "    job_id = client.start_job_run(JobName = glue_job_name) \n",
    "\n",
    "     \n",
    "\n",
    "    #wait for the job to complete \n",
    "\n",
    "    job_state = \"RUNNING\" \n",
    "\n",
    "    while job_state != \"SUCCEEDED\": \n",
    "\n",
    "        time.sleep(60) \n",
    "\n",
    "        status = client.get_job_run(JobName = glue_job_name, RunId = job_id['JobRunId']) \n",
    "\n",
    "        job_state = status['JobRun']['JobRunState'] \n",
    "\n",
    "  \n",
    "\n",
    "    print ('data processing completed') \n",
    "\n",
    "    return f\"GLUE job id: {job_id['JobRunId']}\" \n",
    "\n",
    "  \n",
    "\n",
    "process_data_op = components.func_to_container_op( \n",
    "\n",
    "    process_data, \n",
    "\n",
    "    base_image=BASE_IMAGE,  \n",
    "\n",
    "    packages_to_install =['boto3'] \n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component( \n",
    "\n",
    "    name='model_training_op', \n",
    "\n",
    "    description='model training step', \n",
    "\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step.  \n",
    "\n",
    ") \n",
    "\n",
    "def train_model(bucket: str, key: str, region: str, previous_output: str ) -> str : \n",
    "\n",
    "    import os \n",
    "\n",
    "     \n",
    "\n",
    "    import boto3 \n",
    "\n",
    "    import mlflow \n",
    "\n",
    "    import pandas as pd \n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "    from sklearn.model_selection import train_test_split \n",
    "\n",
    "     \n",
    "\n",
    "    s3 = boto3.client('s3', region_name= region) \n",
    "\n",
    "    response = s3.list_objects (Bucket = bucket, Prefix = key) \n",
    "\n",
    "  \n",
    "\n",
    "    key = response['Contents'][0]['Key'] \n",
    "\n",
    "    s3.download_file ('datalake-demo-dyping', key, \"churn.csv\") \n",
    "\n",
    "     \n",
    "\n",
    "    churn_data = pd.read_csv('churn.csv') \n",
    "\n",
    "        \n",
    "\n",
    "    # Split the dataset into training (80%) and testing (20%). \n",
    "\n",
    "    churn_train, churn_test = train_test_split(churn_data, test_size=0.2) \n",
    "\n",
    "  \n",
    "\n",
    "    churn_train_X = churn_train.loc[:, churn_train.columns != 'exited'] \n",
    "\n",
    "    churn_train_y = churn_train['exited'] \n",
    "\n",
    "  \n",
    "\n",
    "    churn_test_X = churn_test.loc[:, churn_test.columns != 'exited'] \n",
    "\n",
    "    churn_test_y = churn_test['exited'] \n",
    "\n",
    "     \n",
    "\n",
    "    tracking_uri = <<your mlflow tracking server url>> \n",
    "\n",
    "     \n",
    "\n",
    "    mlflow.set_tracking_uri(tracking_uri) \n",
    "\n",
    "    mlflow.set_experiment('Churn Experiment 3') \n",
    "\n",
    "     \n",
    "\n",
    "    with mlflow.start_run(run_name=\"churn_run_2\") as run: \n",
    "\n",
    "        bank_churn_clf = RandomForestClassifier(max_depth=2, random_state=0) \n",
    "\n",
    "        mlflow.sklearn.autolog() \n",
    "\n",
    "        bank_churn_clf.fit(churn_train_X, churn_train_y)  \n",
    "\n",
    "        mlflow.sklearn.log_model(sk_model=bank_churn_clf, artifact_path=\"sklearn-model\", registered_model_name=\"churn-model\") \n",
    "\n",
    "     \n",
    "\n",
    "    print (f\"MLflow run id: {run.info.run_id}\") \n",
    "\n",
    "    return f\"MLflow run id: {run.info.run_id}\" \n",
    "\n",
    "     \n",
    "\n",
    "train_model_op = components.func_to_container_op( \n",
    "\n",
    "    train_model, \n",
    "\n",
    "    base_image=BASE_IMAGE,  \n",
    "\n",
    "    packages_to_install =['boto3', 'mlflow', 'scikit-learn', 'matplotlib'], \n",
    "\n",
    ") \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component( \n",
    "\n",
    "    name='model_download_op', \n",
    "\n",
    "    description='model training step', \n",
    "\n",
    "    base_image=BASE_IMAGE  # you can define the base image here, or when you build in the next step.  \n",
    "\n",
    ") \n",
    "\n",
    "def download_model(model_version: int, previous_output: str ) -> str : \n",
    "\n",
    "    import mlflow \n",
    "\n",
    "    import os \n",
    "\n",
    "    import shutil \n",
    "\n",
    "    import boto3 \n",
    "\n",
    " \n",
    "\n",
    "    model_name = \"churn-model\" \n",
    "\n",
    "    model_version = model_version \n",
    "\n",
    " \n",
    "\n",
    "    tracking_uri = <<your mlflow tracking server>>  \n",
    "\n",
    "     \n",
    "\n",
    "    mlflow.set_tracking_uri(tracking_uri) \n",
    "\n",
    "    mlflow.set_experiment('Churn Experiment 3') \n",
    "\n",
    "     \n",
    "\n",
    "    sk_model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\") \n",
    "\n",
    " \n",
    "\n",
    "    mlflow.sklearn.save_model(sk_model, f\"{model_name}_{model_version}\") \n",
    "\n",
    " \n",
    "\n",
    "    os.mkdir(f\"skserver_{model_name}_release\") \n",
    "\n",
    "     \n",
    "\n",
    "     \n",
    "\n",
    "    src = f\"{model_name}_{model_version}/model.pkl\" \n",
    "\n",
    "    des = f\"skserver_{model_name}_release/model.joblib\" \n",
    "\n",
    " \n",
    "\n",
    "    shutil.copyfile(src, des) \n",
    "\n",
    "     \n",
    "\n",
    "    targetbucket = \"model-deployment-<your initials>\" \n",
    "\n",
    "    prefix = f\"mlflow-models/{model_name}_release\" \n",
    "\n",
    "             \n",
    "\n",
    "    def upload_objects(src_path, bucketname): \n",
    "\n",
    "        s3 = boto3.resource('s3') \n",
    "\n",
    "        my_bucket = s3.Bucket(bucketname) \n",
    "\n",
    "  \n",
    "\n",
    "        for path, dirs, files in os.walk(src_path): \n",
    "\n",
    "            dirs[:] = [d for d in dirs if not d.startswith('.')] \n",
    "\n",
    "             \n",
    "\n",
    "            path = path.replace(\"\\\\\",\"/\") \n",
    "\n",
    "            directory_name = prefix + path.replace(src_path,\"\") \n",
    "\n",
    "            for file in files: \n",
    "\n",
    "                my_bucket.upload_file(os.path.join(path, file), directory_name + \"/\" + file) \n",
    "\n",
    "  \n",
    "\n",
    "    upload_objects (des, targetbucket) \n",
    "\n",
    "     \n",
    "\n",
    "    print (f\"target bucket: {targetbucket}, prefix: {prefix} \") \n",
    "\n",
    "    return f\"target bucket: {targetbucket}, prefix: {prefix} \" \n",
    "\n",
    " \n",
    "\n",
    "model_download_op = components.func_to_container_op( \n",
    "\n",
    "    download_model, \n",
    "\n",
    "    base_image=BASE_IMAGE,  \n",
    "\n",
    "    packages_to_install =['boto3', 'mlflow', 'scikit-learn'], \n",
    "\n",
    ") \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline( \n",
    "\n",
    "  name='bank churn pipeline', \n",
    "\n",
    "  description='Train bank churn model' \n",
    "\n",
    ") \n",
    "\n",
    "def preprocess_train_deploy( \n",
    "\n",
    "        bucket: str = 'datalake-demo-dyping', \n",
    "\n",
    "        glue_job_name: str = 'customer-churn-processing', \n",
    "\n",
    "        region: str = <<aws region>>, \n",
    "\n",
    "        tag: str = '4', \n",
    "\n",
    "        model: str = 'bank_churn_model', \n",
    "\n",
    "        model_version: int = 1, \n",
    "\n",
    "): \n",
    "\n",
    "    precess_data_task = process_data_op(glue_job_name, region).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'us-west-1')) \n",
    "\n",
    "     \n",
    "\n",
    "    model_training_task = train_model_op(bucket,'ml-customer-churn/data/', region, precess_data_task.output).apply(use_aws_secret())  \n",
    "\n",
    "     \n",
    "\n",
    "    model_download_task = model_download_op(model_version, model_training_task.output).apply(use_aws_secret()) \n",
    "\n",
    "     \n",
    "\n",
    "    seldon_config = yaml.load(open(\"bank_churn_deployment.yaml\")) \n",
    "\n",
    "    deploy_op = dsl.ResourceOp( \n",
    "\n",
    "        name=\"seldondeploy\", \n",
    "\n",
    "        k8s_resource=seldon_config, \n",
    "\n",
    "        action = \"apply\", \n",
    "\n",
    "        attribute_outputs={\"name\": \"{.metadata.name}\"}) \n",
    "\n",
    "     \n",
    "\n",
    "    deploy_op.after(model_download_task) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler \n",
    "\n",
    " \n",
    "\n",
    "pipeline_filename = 'bank_churn_pipeline.tar.gz' \n",
    "\n",
    "compiler.Compiler().compile(preprocess_train_deploy, pipeline_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client() \n",
    "\n",
    "experiment = client.create_experiment(name='data_experiment', namespace='admin') \n",
    "\n",
    " \n",
    "\n",
    "arguments = {'model_version':1} \n",
    "\n",
    "pipeline_func = preprocess_train_deploy \n",
    "\n",
    "run_name = pipeline_func.__name__ + '_run' \n",
    "\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
