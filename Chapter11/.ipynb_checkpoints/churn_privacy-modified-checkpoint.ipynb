{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nominated-navigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opacus\n",
      "  Downloading opacus-0.14.0-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 735 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/site-packages (from opacus) (1.19.5)\n",
      "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.9/site-packages (from opacus) (1.8.1)\n",
      "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.9/site-packages (from opacus) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch>=1.3->opacus) (3.7.4.3)\n",
      "Installing collected packages: opacus\n",
      "Successfully installed opacus-0.14.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elementary-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interested-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prime-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnDataset(Dataset):\n",
    " \n",
    "    def __init__(self, csv_file):\n",
    "  \n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        df = df.drop([\"Surname\", \"CustomerId\", \"RowNumber\"], axis=1)\n",
    "\n",
    "        # Grouping variable names\n",
    "        self.categorical = [\"Geography\", \"Gender\"]\n",
    "        self.target = \"Exited\"\n",
    "\n",
    "        # One-hot encoding of categorical variables\n",
    "        self.churn_frame = pd.get_dummies(df, prefix=self.categorical)\n",
    "\n",
    "        # Save target and predictors\n",
    "        self.X = self.churn_frame.drop(self.target, axis=1)\n",
    "        self.y = self.churn_frame[\"Exited\"]\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_array  = scaler.fit_transform(self.X)\n",
    "        self.X = pd.DataFrame(X_array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.churn_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return [self.X.iloc[idx].values, self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "joint-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CHURN_model():\n",
    "    model = nn.Sequential(nn.Linear(13, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 1)) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "noble-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(csv_file, batch_size):\n",
    "     # Load dataset\n",
    "    dataset = ChurnDataset(csv_file)\n",
    "\n",
    "    # Split into training and test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return trainloader, testloader, trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "breeding-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, net, optimizer, n_epochs=100):\n",
    "     \n",
    "    device = \"cpu\"\n",
    "\n",
    "    # Define the model\n",
    "    #net = get_CHURN_model()\n",
    "    net = net.to(device)\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss() \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    # Train the net\n",
    "    loss_per_iter = []\n",
    "    loss_per_batch = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save loss to plot\n",
    "            running_loss += loss.item()\n",
    "            loss_per_iter.append(loss.item())\n",
    "\n",
    "        \n",
    "        print(\"Epoch {} - Training loss: {}\".format(epoch, running_loss/len(trainloader))) \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cooked-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/churn.csv\"\n",
    "\n",
    "trainloader, testloader, train_ds, test_ds = get_dataloader(csv_file, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-physiology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.42998170768842103\n",
      "Epoch 1 - Training loss: 0.3585398425348103\n",
      "Epoch 2 - Training loss: 0.34484875975176693\n"
     ]
    }
   ],
   "source": [
    "net = get_CHURN_model()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "\n",
    "model = train(trainloader, net, optimizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-interview",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
