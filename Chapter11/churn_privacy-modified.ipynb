{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nominated-navigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opacus in /usr/local/lib/python3.9/site-packages (0.14.0)\n",
      "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.9/site-packages (from opacus) (1.6.0)\n",
      "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.9/site-packages (from opacus) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/site-packages (from opacus) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch>=1.3->opacus) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elementary-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interested-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prime-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnDataset(Dataset):\n",
    " \n",
    "    def __init__(self, csv_file):\n",
    "  \n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        df = df.drop([\"Surname\", \"CustomerId\", \"RowNumber\"], axis=1)\n",
    "\n",
    "        # Grouping variable names\n",
    "        self.categorical = [\"Geography\", \"Gender\"]\n",
    "        self.target = \"Exited\"\n",
    "\n",
    "        # One-hot encoding of categorical variables\n",
    "        self.churn_frame = pd.get_dummies(df, prefix=self.categorical)\n",
    "\n",
    "        # Save target and predictors\n",
    "        self.X = self.churn_frame.drop(self.target, axis=1)\n",
    "        self.y = self.churn_frame[\"Exited\"]\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_array  = scaler.fit_transform(self.X)\n",
    "        self.X = pd.DataFrame(X_array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.churn_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return [self.X.iloc[idx].values, self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "joint-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CHURN_model():\n",
    "    model = nn.Sequential(nn.Linear(13, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 1)) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "noble-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(csv_file, batch_size):\n",
    "     # Load dataset\n",
    "    dataset = ChurnDataset(csv_file)\n",
    "\n",
    "    # Split into training and test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return trainloader, testloader, trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "breeding-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, net, optimizer, n_epochs=100):\n",
    "     \n",
    "    device = \"cpu\"\n",
    "\n",
    "    # Define the model\n",
    "    #net = get_CHURN_model()\n",
    "    net = net.to(device)\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss() \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    # Train the net\n",
    "    loss_per_iter = []\n",
    "    loss_per_batch = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save loss to plot\n",
    "            running_loss += loss.item()\n",
    "            loss_per_iter.append(loss.item())\n",
    "\n",
    "        \n",
    "        print(\"Epoch {} - Training loss: {}\".format(epoch, running_loss/len(trainloader))) \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cooked-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"data/churn.csv\"\n",
    "\n",
    "trainloader, testloader, train_ds, test_ds = get_dataloader(csv_file, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "traditional-physiology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.4372446881607175\n",
      "Epoch 1 - Training loss: 0.3615658536553383\n",
      "Epoch 2 - Training loss: 0.34411114174872637\n",
      "Epoch 3 - Training loss: 0.3393215463496745\n",
      "Epoch 4 - Training loss: 0.3357653792947531\n",
      "Epoch 5 - Training loss: 0.33380249571055176\n",
      "Epoch 6 - Training loss: 0.33079878855496647\n",
      "Epoch 7 - Training loss: 0.3274117988534272\n",
      "Epoch 8 - Training loss: 0.32857229094952345\n",
      "Epoch 9 - Training loss: 0.3246911917813122\n",
      "Epoch 10 - Training loss: 0.32426827838644384\n",
      "Epoch 11 - Training loss: 0.3234902940690517\n",
      "Epoch 12 - Training loss: 0.3196106592193246\n",
      "Epoch 13 - Training loss: 0.31688680476509035\n",
      "Epoch 14 - Training loss: 0.3178471109829843\n",
      "Epoch 15 - Training loss: 0.31471656495705247\n",
      "Epoch 16 - Training loss: 0.31377251804806294\n",
      "Epoch 17 - Training loss: 0.31305785235017536\n",
      "Epoch 18 - Training loss: 0.3117420727387071\n",
      "Epoch 19 - Training loss: 0.30834818808361886\n",
      "Epoch 20 - Training loss: 0.3049893509596586\n",
      "Epoch 21 - Training loss: 0.30424610823392867\n",
      "Epoch 22 - Training loss: 0.3048344370909035\n",
      "Epoch 23 - Training loss: 0.30211211731657384\n",
      "Epoch 24 - Training loss: 0.29957143450155854\n",
      "Epoch 25 - Training loss: 0.29879740057513116\n",
      "Epoch 26 - Training loss: 0.29686895832419397\n",
      "Epoch 27 - Training loss: 0.29515311839058994\n",
      "Epoch 28 - Training loss: 0.29490576605312524\n",
      "Epoch 29 - Training loss: 0.294009745772928\n",
      "Epoch 30 - Training loss: 0.29006671318784355\n",
      "Epoch 31 - Training loss: 0.28867378979921343\n",
      "Epoch 32 - Training loss: 0.28815564382821324\n",
      "Epoch 33 - Training loss: 0.28672573301009835\n",
      "Epoch 34 - Training loss: 0.28491816902533174\n",
      "Epoch 35 - Training loss: 0.2821703376248479\n",
      "Epoch 36 - Training loss: 0.281001459620893\n",
      "Epoch 37 - Training loss: 0.280216332571581\n",
      "Epoch 38 - Training loss: 0.278858354780823\n",
      "Epoch 39 - Training loss: 0.27852872586809097\n",
      "Epoch 40 - Training loss: 0.27627975144423544\n",
      "Epoch 41 - Training loss: 0.27805697564035653\n",
      "Epoch 42 - Training loss: 0.27184095708653333\n",
      "Epoch 43 - Training loss: 0.2713604767341167\n",
      "Epoch 44 - Training loss: 0.2723252744413912\n",
      "Epoch 45 - Training loss: 0.26584482165053486\n",
      "Epoch 46 - Training loss: 0.26611972036771475\n",
      "Epoch 47 - Training loss: 0.2634416268672794\n",
      "Epoch 48 - Training loss: 0.2629359430167824\n",
      "Epoch 49 - Training loss: 0.2603476152289659\n"
     ]
    }
   ],
   "source": [
    "net = get_CHURN_model()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "\n",
    "model = train(trainloader, net, optimizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "valuable-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_per_sample_grad_norm = 1.5\n",
    "sample_rate = batch_size/len(train_ds)\n",
    "noise_multiplier = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "concerned-directory",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/opacus/privacy_engine.py:645: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.5503569555468857\n",
      "Epoch 1 - Training loss: 0.5270387591794133\n",
      "Epoch 2 - Training loss: 0.5277176320552825\n",
      "Epoch 3 - Training loss: 0.5181171683594584\n",
      "Epoch 4 - Training loss: 0.5345764994621277\n",
      "Epoch 5 - Training loss: 0.5284739407710731\n",
      "Epoch 6 - Training loss: 0.528376258444041\n",
      "Epoch 7 - Training loss: 0.5217884532175958\n",
      "Epoch 8 - Training loss: 0.5271113646216691\n",
      "Epoch 9 - Training loss: 0.5200825051404536\n",
      "Epoch 10 - Training loss: 0.5151959240902215\n",
      "Epoch 11 - Training loss: 0.50616883514449\n",
      "Epoch 12 - Training loss: 0.5010638677980751\n",
      "Epoch 13 - Training loss: 0.5051685086451471\n",
      "Epoch 14 - Training loss: 0.5023456629365682\n",
      "Epoch 15 - Training loss: 0.4927736475132406\n",
      "Epoch 16 - Training loss: 0.4892598757520318\n",
      "Epoch 17 - Training loss: 0.5009411306120455\n",
      "Epoch 18 - Training loss: 0.5002795286476612\n",
      "Epoch 19 - Training loss: 0.499312580563128\n",
      "Epoch 20 - Training loss: 0.5023985943291336\n",
      "Epoch 21 - Training loss: 0.4957516428083181\n",
      "Epoch 22 - Training loss: 0.5104077772237361\n",
      "Epoch 23 - Training loss: 0.5040733776986599\n",
      "Epoch 24 - Training loss: 0.5197800094727427\n",
      "Epoch 25 - Training loss: 0.525708153937012\n",
      "Epoch 26 - Training loss: 0.5229357150383294\n",
      "Epoch 27 - Training loss: 0.5214119812939316\n",
      "Epoch 28 - Training loss: 0.5318625300657004\n",
      "Epoch 29 - Training loss: 0.5345330509822815\n",
      "Epoch 30 - Training loss: 0.523054621880874\n",
      "Epoch 31 - Training loss: 0.5216699715703725\n",
      "Epoch 32 - Training loss: 0.5335271127521992\n",
      "Epoch 33 - Training loss: 0.5366211591055616\n",
      "Epoch 34 - Training loss: 0.5359228259883821\n",
      "Epoch 35 - Training loss: 0.5361567279789596\n",
      "Epoch 36 - Training loss: 0.5392460661008954\n",
      "Epoch 37 - Training loss: 0.5291419992223382\n",
      "Epoch 38 - Training loss: 0.5363238048041239\n",
      "Epoch 39 - Training loss: 0.5308523774554488\n",
      "Epoch 40 - Training loss: 0.5302893952932208\n",
      "Epoch 41 - Training loss: 0.5232803815975785\n",
      "Epoch 42 - Training loss: 0.5286133877350949\n",
      "Epoch 43 - Training loss: 0.5281467778608203\n",
      "Epoch 44 - Training loss: 0.5292599899694324\n",
      "Epoch 45 - Training loss: 0.5317839432973415\n",
      "Epoch 46 - Training loss: 0.534774599922821\n",
      "Epoch 47 - Training loss: 0.5246870778501034\n",
      "Epoch 48 - Training loss: 0.528704051207751\n",
      "Epoch 49 - Training loss: 0.5301720108371228\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "\n",
    "net = get_CHURN_model()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "\n",
    "privacy_engine = PrivacyEngine(\n",
    "    net,\n",
    "    max_grad_norm=max_per_sample_grad_norm,\n",
    "    noise_multiplier = noise_multiplier,\n",
    "    sample_rate = sample_rate,\n",
    ")\n",
    "\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "model = train(trainloader, net, optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "choice-picnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ε = 6.39, δ = 1e-06\n"
     ]
    }
   ],
   "source": [
    "epsilon, best_alpha = privacy_engine.get_privacy_spent()\n",
    "print (f\" ε = {epsilon:.2f}, δ = {privacy_engine.target_delta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-mineral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
